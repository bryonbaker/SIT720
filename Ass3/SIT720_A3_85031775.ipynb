{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdb9eef-2101-4dec-8a2e-1d6c0f48f270",
   "metadata": {},
   "source": [
    "# SIT720 Assignment 3\n",
    "Bryon Baker\n",
    "\n",
    "Student number: 85031775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb60c4-b451-4de8-b3b9-bad1e76a1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTO: Uncomment this before submitting the assignment\n",
    "\n",
    "#!pip3 install numpy==1.22.3\n",
    "#!pip3 install pandas== 1.4.2\n",
    "!pip3 install scikit-learn==1.0.2\n",
    "#!pip3 install statsmodels==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b8bd9-33bb-4d1c-a3c9-6a0c2a19aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import timeit\n",
    "import random\n",
    "import pandas as pd # dataframe manipulation\n",
    "import numpy as np # linear algebra\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# Configure ssl for unverified content so we can load a dataset from an unknown source (github).\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c4196b-c033-4c02-800e-35daa35b6b24",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Load and explore the training dataset. Explain the steps that you have taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236ba04-5132-4871-908a-e088ff369173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOME = os.getenv('HOME')\n",
    "print(HOME)\n",
    "\n",
    "train_url = 'https://raw.githubusercontent.com/bryonbaker/datasets/main/SIT720/Ass3/ac_train_data.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/bryonbaker/datasets/main/SIT720/Ass3/ac_test_data.csv'\n",
    "\n",
    "train_path = HOME+\"/datasets/SIT720/Ass3/ac_train_data.csv\"\n",
    "test_path = HOME+\"/datasets/SIT720/Ass3/ac_test_data.csv\"\n",
    "\n",
    "#\n",
    "# Work out if the datasets are local. If not use a remote url. Preference is local.\n",
    "#\n",
    "print(\"Load datasets from local or remote resource:\")\n",
    "print(\"=\"*50)\n",
    "if os.path.isfile(train_path):\n",
    "    print(f\"Training data is local: {train_path}\")\n",
    "    training_data = train_path\n",
    "else:\n",
    "    print(\"Training data is remote: {}\".format(train_url))\n",
    "    training_data = train_url\n",
    "\n",
    "if os.path.isfile(test_path):\n",
    "    print(f\"Test data is local: {test_path}\")\n",
    "    test_data = test_path\n",
    "else:\n",
    "    print(\"Test data is remote: {}\".format(test_url))\n",
    "    test_data = test_url\n",
    "print()\n",
    "\n",
    "# Load the datasets from either local or remote.\n",
    "eng_train_df = pd.read_csv(training_data)\n",
    "eng_val_df = pd.read_csv(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62b4e6-7367-4336-a02d-aa31c53aaef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns not used in the assignment\n",
    "\n",
    "# Drop the sequence number (column 0) from the test dataset. Column 0 is unlabeled so use the index number.\n",
    "eng_train_df = eng_train_df.drop(eng_train_df.columns[[0]],axis=1)\n",
    "eng_val_df = eng_val_df.drop(eng_val_df.columns[[0]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccbbdea-73d5-4835-b623-11eee69a287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encode the days of week.\n",
    "\n",
    "oldCol = \"dayofweek\"\n",
    "newCol = \"dayofweeknum\"\n",
    "dayMap = {'Sun' : 0, 'Mon': 1, \"Tue\" : 2, \"Wed\" : 3, \"Thu\" : 4, \"Fri\" : 5, \"Sat\" : 6 }    # How to map the values\n",
    "\n",
    "eng_train_df[newCol] = eng_train_df[oldCol].map(dayMap)    # Adds a new column with proper boolean values\n",
    "eng_train_df = eng_train_df.drop([oldCol], axis=1)    # Drop the old column before renaming the new column to the name just dropped\n",
    "eng_train_df = eng_train_df.rename(columns={newCol : oldCol})\n",
    "      \n",
    "eng_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9b9dd-fc98-4a32-887c-92c362ad7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encode the dayof week in the test dataset.\n",
    "eng_val_df[newCol] = eng_val_df[oldCol].map(dayMap)    # Adds a new column with proper boolean values\n",
    "eng_val_df = eng_val_df.drop([oldCol], axis=1)    # Drop the old column before renaming the new column to the name just dropped\n",
    "eng_val_df = eng_val_df.rename(columns={newCol : oldCol})\n",
    "      \n",
    "eng_val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff615579-3dbf-4e78-a2d9-b8eff9982a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out the X and y from the datasets\n",
    "eng_train_X = eng_train_df.drop(['ac'], axis=1)\n",
    "eng_train_y = eng_train_df[\"ac\"]\n",
    "eng_val_X = eng_val_df.drop(['ac'], axis=1)\n",
    "eng_val_y = eng_val_df['ac']\n",
    "\n",
    "print(\"Training Dataset\")\n",
    "print(f\"{eng_train_X.head()}\\n\")\n",
    "print(f\"{eng_train_y.head()}\\n\")\n",
    "\n",
    "print(\"Testing Dataset\")\n",
    "print(f\"{eng_val_X.head()}\\n\")\n",
    "print(f\"{eng_val_y.head()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa765f5-70a4-4892-b8dd-9f00d470591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Analysis of Dataset\")\n",
    "print(eng_train_X.describe())\n",
    "print()\n",
    "\n",
    "print(\"Summary Analysis of the Class\")\n",
    "print(eng_train_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95373a27-80d3-4c31-ae22-16b658cc1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variable Correlation Matrix\")\n",
    "corr_X = eng_train_df.corr()\n",
    "sn.set(rc = {'figure.figsize':(15,8)})\n",
    "sn.heatmap(corr_X, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f561b-ee74-4c26-8af2-c9796e58ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snippet coutesy of: https://benjaminobi.medium.com/5-minutes-tutorial-on-how-to-compute-and-visualize-the-covariance-matrix-2597ab98d9ee#:~:text=The%20covariance%20matrix%20gives%20the,that%20displays%20the%20correlation%20coefficients.\n",
    "#\n",
    "stdsc = StandardScaler() \n",
    "X_std = stdsc.fit_transform(eng_train_df[eng_train_df.columns].iloc[:,range(0,len(eng_train_df.columns))].values)\n",
    "cov_mat =np.cov(X_std.T)\n",
    "plt.figure(figsize=(12,12))\n",
    "sn.set(font_scale=1.5)\n",
    "hm = sn.heatmap(cov_mat,\n",
    "                 cbar=True,\n",
    "                 annot=True,\n",
    "                 square=True,\n",
    "                 fmt='.2f',\n",
    "                 annot_kws={'size': 12},\n",
    "                 cmap='coolwarm',                 \n",
    "                 yticklabels=eng_train_df.columns,\n",
    "                 xticklabels=eng_train_df.columns)\n",
    "plt.title('Covariance matrix with correlation coefficients', size = 18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2ba0a-d72f-467e-aa4e-c3c4dc7ed6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an auto correlation to review the lag .\n",
    "# A spike close to zero is evidence against autocorrelation\n",
    "# A large spike is evidence of auto correlation\n",
    "autocorrelation_plot(eng_train_df[\"load\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3553a6d0-3431-4710-bb4a-939b0062fc0e",
   "metadata": {},
   "source": [
    "There is a positive correlation within approximately the first 100,000 lags. \n",
    "\n",
    "The spikes at what appears to be < 1000 indicates that load is correlated for small windows of time. I.e. For a short period of time, when load is going up it continues to go up, and when load is going down it continues to go down.\n",
    "\n",
    "Ref: https://www.dummies.com/article/technology/information-technology/data-science/big-data/autocorrelation-plots-graphical-technique-for-statistical-data-141241/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41076bd-a2e0-4f7d-838f-50fdd17baf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pairplot\n",
    "# Commented out because this plot takes a VERY long time to run\n",
    "# TODO: Uncomment before submitting assignment.\n",
    "#sn.pairplot(eng_train_df[eng_train_X.columns], height=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb9c60-33d6-4fc8-b6bb-3ad54842fe5c",
   "metadata": {},
   "source": [
    "### Visualise the relationship between load "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73789ee-f422-42ac-9f04-66bb3f694364",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Analyse the importance of the feautues for predicting air conditioner status using two different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2825a2a-fb9b-450f-927d-aa95ca8c9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "def plotFeaturePerformance( fs, ticks, title ):\n",
    "    # what are scores for the features\n",
    "    for i in range(len(fs.scores_)):\n",
    "        print('Feature %d: %f' % (i, fs.scores_[i]))\n",
    "        \n",
    "    # plot the scores\n",
    "    plt.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"K-Best Score\")\n",
    "    plt.title(label=title, loc=\"center\")\n",
    "    plt.xticks(np.arange(len(ticks)), labels=ticks, rotation=70)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def selectFeatures( X_train, y_pred, criteria, num_features ):\n",
    "    fs = SelectKBest(criteria, k=num_features)\n",
    "    fs.fit(X_train,y_pred)\n",
    "    new_x_train = fs.transform(X_train)\n",
    "    \n",
    "    return new_x_train, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a852f94-0d09-4964-aba1-a7506ec8dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of features to find.\n",
    "num_features = 5\n",
    "print(\"Column names:\\n{}\".format(eng_train_X.columns))\n",
    "new_x_train1, fs1 = selectFeatures( eng_train_X, eng_train_y, f_classif, num_features )\n",
    "new_x_train2, fs2 = selectFeatures( eng_train_X, eng_train_y, f_regression, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618cabd-cea1-425c-8b29-f01abe46f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature selection charts.\n",
    "plotFeaturePerformance( fs1, eng_train_X.columns, \"ANOVA Feature Selection\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691016d-82c8-4346-b2a6-0d8643669ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotFeaturePerformance( fs2, eng_train_X.columns, \"Regression Feature Selection\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2109c6-7f29-401f-be2d-13ea4ec4c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "Xnorm = scale(eng_train_X)\n",
    "\n",
    "# Perform the Principal Component Analysis. The number of components is the number of coumns in the dataset.\n",
    "# The number of components is the minimum of the number of data elements and festures \n",
    "n_components=min(eng_train_X.shape[0], eng_train_X.shape[1])\n",
    "print(\"Number of comoponets: {}\".format(n_components))\n",
    "n_components=7\n",
    "pca = PCA(n_components)\n",
    "Xnew = pca.fit_transform(Xnorm)\n",
    "print(\"PCA details => {}\".format(pca))\n",
    "\n",
    "print(pca.get_params())\n",
    "\n",
    "print('='*50)\n",
    "print(\"Explained Variance:\")\n",
    "# Calculate the amount of variance explained by each PC\n",
    "var= pca.explained_variance_ratio_\n",
    "print(var)\n",
    "\n",
    "print('='*50)\n",
    "print(\"Cumulative Explained Variance:\")\n",
    "\n",
    "#cum_var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "cum_var=np.cumsum(pca.explained_variance_ratio_*100)\n",
    "print(cum_var)\n",
    "\n",
    "dimensions = {}\n",
    "\n",
    "# Find the minimum dimensions that capture 85% and 95% respectively\n",
    "num_dimensions = 0\n",
    "for value in cum_var:\n",
    "    if value >= 85:\n",
    "        dimensions[value]=num_dimensions\n",
    "        break\n",
    "    num_dimensions += 1\n",
    "\n",
    "num_dimensions = 0\n",
    "for value in cum_var:\n",
    "    if value >= 95:\n",
    "        dimensions[value]=num_dimensions\n",
    "        break\n",
    "    num_dimensions += 1\n",
    "\n",
    "print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1115e28-806a-4ee3-9948-e2bf077e338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_num = a_list = list(range(1, n_components+1))\n",
    "plt.plot(pca_num, cum_var, marker='.')\n",
    "plt.xlabel(\"Principal components\")\n",
    "plt.ylabel(\"Variance captured\")\n",
    "plt.title(label=\"Captured Variance vs. Latent Dimensionality\", loc=\"center\")\n",
    "plt.show()\n",
    "\n",
    "for key,value in dimensions.items():\n",
    "    print(\"The minimum dimension that captures at least {:.0f}% variance is: {}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc5db0-90d8-424f-9bc3-be8284e15b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)\n",
    "print(type(pca.components_))\n",
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4309f-7098-4cb8-9bc8-3c21ee8c4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_groups = pca.components_.shape[1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.bar(index, np.absolute(pca.components_[0,:]), \n",
    "        bar_width,\n",
    "        color='r',\n",
    "        label='Component 1')\n",
    "\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Feature number')\n",
    "plt.title('Principal Component Scores')\n",
    "#plt.xticks(index + bar_width / 2, ('A', 'B', 'C', 'D', 'E'))\n",
    "plt.xticks(index+bar_width/2, eng_train_X.columns)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ac886-c524-470b-a109-566b3ada1e8e",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Now that we have analysed the features and their importance we will create a new training dataset that has the features we want in the final train/test dataset and a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181ed96-f533-4c11-8318-a743c96a5583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff94ccf6-fbba-484a-b879-9479bcd653fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3f0db-7455-450c-ac55-392e504648c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5727423-abb5-4e4b-b322-0ec710025c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cfc9d-0d98-4f1c-bc85-015efe23696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = eng_train_X\n",
    "train_y = eng_train_y\n",
    "val_X = eng_val_X\n",
    "val_y = eng_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56e106-0bba-46ce-81b0-8034dde906f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff53e31-b932-48b6-87aa-ee012cfdc27d",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Based on the training data, create three different supervised machine learning models excet any ensemble approach for predicting air conditioner status\n",
    "   1.\tReport performance score using a suitable metric on the test data. Is it possible that the presented result is an underfitted or overfitted one? Justify.  \n",
    "   2.\tJustify different design decisions for each ML model used to answer this question.\n",
    "   3.\tHave you optimised any hyper-parameters for each ML model? What are they? Why have you done that? Explain. \n",
    "   4.\tFinally, make a recommendation based on the reported results and justify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942abb84-9993-45e3-822c-f2fe4cf721ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71477431-5042-4866-96c3-ac3489d783a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def current_milli_time():\n",
    "    return round(time.time() * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83646b-fdb6-463b-9a32-979a47cc6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def display_stats(y_test, y_pred):\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # Model Precision: what percentage of positive tuples are labeled as such?\n",
    "    print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "    # Model Recall: what percentage of positive tuples are labelled as such?\n",
    "    print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "    #Calculate F1 Score\n",
    "    print(\"F1 Score:\",metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "    #Calculate Mean Absolute Error\n",
    "    print(\"Mean Absolute Error:\",metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    # kappa\n",
    "    print(\"Cohens kappa:\", metrics.cohen_kappa_score(y_test, y_pred))\n",
    "\n",
    "    # ROC AUC\n",
    "    print(\"ROC AUC:\", metrics.roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6ade4-7cb0-4f62-9b22-c93ecf7fd33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model( model, validation_dataset_X, validation_dataset_y ):\n",
    "    print(f\"Validating model: {model}\")\n",
    "    y_pred = model.predict( validation_dataset_X )\n",
    "    # Find the performance info\n",
    "    accuracy = metrics.accuracy_score(validation_dataset_y, y_pred)\n",
    "    precision = metrics.precision_score(validation_dataset_y, y_pred)\n",
    "    recall = metrics.recall_score(validation_dataset_y, y_pred)\n",
    "    f1 = metrics.f1_score(validation_dataset_y, y_pred)\n",
    "\n",
    "    print(f\"Accuracy {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f118b1-1ada-44cf-a263-2376f6ddcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Some of the hyper parameter combinations are invalid. Supress the warnings.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7dbb24-5d90-4ccc-9458-d1c202c9b713",
   "metadata": {},
   "source": [
    "### Test using the Decision Tree Model\n",
    "The decision tree is a fast and explainable model. The most common hyperparameters to tune are the max leaf nodes, criterion and the minimum samples before you can split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13202e-f05e-45ee-9b67-d34f11e0c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_decision_tree( df_X, df_y, c, m, s ):\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    score = []\n",
    "    for train_index, test_index in skf.split(df_X, df_y):\n",
    "        X_tr = df_X.iloc[train_index]\n",
    "        X_val = df_X.iloc[test_index]\n",
    "        y_tr = df_y.iloc[train_index]\n",
    "        y_val = df_y.iloc[test_index]\n",
    "\n",
    "        # Train the model with the training split\n",
    "        model = DecisionTreeClassifier(criterion = c, max_leaf_nodes=m, min_samples_split=s)\n",
    "        start = timeit.default_timer()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        stop = timeit.default_timer()\n",
    "        train_time = stop - start\n",
    "\n",
    "        # Now work with the testing split (vaidation set) and measure the performance\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Find the performance info\n",
    "        accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "        precision = metrics.precision_score(y_val, y_pred)\n",
    "        recall = metrics.recall_score(y_val, y_pred)\n",
    "        f1 = metrics.f1_score(y_val, y_pred)\n",
    "\n",
    "        # Store the results so we can average them after all the splits have been run.\n",
    "        score.append([accuracy, precision, recall, f1, train_time])\n",
    "        # print(f\"{[accuracy, precision, recall, f1, train_time]}\")\n",
    "    return model, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1bedc-1b84-42bb-8ba8-261bb51b6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_leaf_nodes = [10, 50, 100, 500]\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "min_samples_split = [2,3,4,8]\n",
    "\n",
    "score = []\n",
    "avg_score= []\n",
    "for c in criterion:\n",
    "    for m in max_leaf_nodes:     \n",
    "        for s in min_samples_split:   \n",
    "            _, score = perform_decision_tree( train_X, train_y, c, m, s )\n",
    "\n",
    "            # print(f\"score: {score}\")\n",
    "            averages = np.array(score).mean(axis=0)\n",
    "            \n",
    "            print(f\"{c} {m} {s}: Averages: {averages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7da719-adeb-4224-947e-21b16fea39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, score = perform_decision_tree( train_X, train_y, 'gini', 500, 2 )\n",
    "print(f\"Trained results: {np.array(score).mean(axis=0)}\")\n",
    "print(\"Validating model with validation dataset:\")\n",
    "validate_model( model, val_X, val_y )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a780fc-c9ee-419b-b300-2f3bf537ba7e",
   "metadata": {},
   "source": [
    "### Test Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d8b80-3a31-4fd2-9c00-5c57522479e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logistic_regression(df_X, df_y, p, s, c):\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    score = []\n",
    "    for train_index, test_index in skf.split(df_X, df_y):\n",
    "        X_tr = df_X.iloc[train_index]\n",
    "        X_val = df_X.iloc[test_index]\n",
    "        y_tr = df_y.iloc[train_index]\n",
    "        y_val = df_y.iloc[test_index]\n",
    "\n",
    "        # Train the model with the training split\n",
    "        model = LogisticRegression(max_iter=10000, penalty=p, solver=s, C=int(c))\n",
    "        start = timeit.default_timer()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        stop = timeit.default_timer()\n",
    "        train_time = stop - start\n",
    "\n",
    "        # Now work with the testing split (vaidation set) and measure the performance\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Find the performance info\n",
    "        accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "        precision = metrics.precision_score(y_val, y_pred)\n",
    "        recall = metrics.recall_score(y_val, y_pred)\n",
    "        f1 = metrics.f1_score(y_val, y_pred)\n",
    "\n",
    "        # Store the results so we can average them after all the splits have been run.\n",
    "        score.append([accuracy, precision, recall, f1, train_time])\n",
    "        #print(f\"{[accuracy, precision, recall, f1]}\")\n",
    "\n",
    "    return model, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128330c1-ef92-4e95-ad6e-9e72ac056936",
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['none','l2']     # have left out none because of feedback form previous runs and the fact it generates a ton of incompatable param warnings\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "i = 1\n",
    "avg_score= []\n",
    "for p in penalty:\n",
    "    for s in solver:\n",
    "        for c in C:\n",
    "            if p == 'none' and s == 'liblinear':\n",
    "                continue        # These are incompatible hyperparameters so skip them\n",
    "            if p == 'l2' and c < 1.0:\n",
    "                continue        # These are incompatible hyperparameters so skip them\n",
    "             \n",
    "            _, score = perform_logistic_regression( train_X, train_y, p, s, c )\n",
    "\n",
    "            averages = np.array(score).mean(axis=0)\n",
    "            \n",
    "            print(f\"Averages: penalty: {p} solver: {s} C: {c} {averages}\")\n",
    "            \n",
    "            #avg_score.append([p, s, c, averages])\n",
    "            #print(f\"\\niteration: {i}: {avg_score}\")\n",
    "            i += 1\n",
    "print(\"complete:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a1dc4-8c75-4a33-88b8-29db76feb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, score = perform_logistic_regression( train_X, train_y, 'l2', 'lbfgs', 1.0 )\n",
    "print(f\"Trained results: {np.array(score).mean(axis=0)}\")\n",
    "print(\"Validating model with validation dataset:\")\n",
    "validate_model( model, val_X, val_y )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbbe8d-2f4c-4c82-aba7-55570b0beac2",
   "metadata": {},
   "source": [
    "### Test the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd2585-9e65-4831-98a1-e067fc31ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_svm(df_X, df_y, c):\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    score = []\n",
    "    for train_index, test_index in skf.split(df_X, df_y):\n",
    "        X_tr = df_X.iloc[train_index]\n",
    "        X_val = df_X.iloc[test_index]\n",
    "        y_tr = df_y.iloc[train_index]\n",
    "        y_val = df_y.iloc[test_index]\n",
    "\n",
    "        # Train the model with the training split\n",
    "        model = SVC(C=c)\n",
    "        start = timeit.default_timer()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        stop = timeit.default_timer()\n",
    "        train_time = stop - start\n",
    "\n",
    "        # Now work with the testing split (vaidation set) and measure the performance\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        # Find the performance info\n",
    "        accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "        precision = metrics.precision_score(y_val, y_pred)\n",
    "        recall = metrics.recall_score(y_val, y_pred)\n",
    "        f1 = metrics.f1_score(y_val, y_pred)\n",
    "\n",
    "        # Store the results so we can average them after all the splits have been run.\n",
    "        score.append([accuracy, precision, recall, f1, train_time])\n",
    "        print(f\"{[accuracy, precision, recall, f1, train_time]}\")\n",
    "        \n",
    "    return model, score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c7f3784-d058-44e7-aea5-9518ab3bf0a5",
   "metadata": {},
   "source": [
    "score = []\n",
    "C = [0.001, 1, 100]\n",
    "for c in C:\n",
    "    print(f\"Training with C = {c}\")\n",
    "    _, score = perform_svm( train_X, train_y, c )\n",
    "\n",
    "    averages = np.array(score).mean(axis=0)\n",
    "    print(f\"Averages: C: {c} {averages}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f6974ea-8b9a-4970-b4cd-d7428d908c1b",
   "metadata": {},
   "source": [
    "model, score = perform_svm( train_X, train_y, 100 )\n",
    "print(f\"Trained results: {np.array(score).mean(axis=0)}\")\n",
    "print(\"Validating model with validation dataset:\")\n",
    "validate_model( model, val_X, val_y )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529eb0f4-c931-408b-86bb-a6288ce756d8",
   "metadata": {},
   "source": [
    "## Tune the Selected Model\n",
    "Now we have an idea of the best model algorithm, a benchmark of performance. Drop some of the less-important features, retrain the model and test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc5b25-1fa8-4727-9673-8fbb0847f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have an idea of the best model algorithm, a benchmark of performance. Drop some of the less-important features, retrain the model and test the performance.\n",
    "train_X = eng_train_X\n",
    "train_X = train_X.drop(['dif','nonlinear','hurst','dayofweek'], axis=1)\n",
    "print(train_X.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9353cd-83fe-4b24-9a15-de4affbd4558",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb903ab-5a31-49b2-ba3e-157ad6528a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "max_leaf_nodes = [500]\n",
    "criterion = [\"gini\"]\n",
    "min_samples_split = [2]\n",
    "\n",
    "score = []\n",
    "avg_score= []\n",
    "for c in criterion:\n",
    "    for m in max_leaf_nodes:     \n",
    "        for s in min_samples_split:  \n",
    "            for train_index, test_index in skf.split(train_X, train_y):\n",
    "                X_tr = train_X.iloc[train_index]\n",
    "                X_val = train_X.iloc[test_index]\n",
    "                y_tr = train_y.iloc[train_index]\n",
    "                y_val = train_y.iloc[test_index]\n",
    "\n",
    "                # Train the model with the training split\n",
    "                model = DecisionTreeClassifier(criterion = c, max_leaf_nodes=m, min_samples_split=s)\n",
    "                start = timeit.default_timer()\n",
    "                model.fit(X_tr, y_tr)\n",
    "                stop = timeit.default_timer()\n",
    "                train_time = stop - start\n",
    "\n",
    "                # Now work with the testing split (vaidation set) and measure the performance\n",
    "                y_pred = model.predict(X_val)\n",
    "\n",
    "                # Find the performance info\n",
    "                accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "                precision = metrics.precision_score(y_val, y_pred)\n",
    "                recall = metrics.recall_score(y_val, y_pred)\n",
    "                f1 = metrics.f1_score(y_val, y_pred)\n",
    "\n",
    "                # Store the results so we can average them after all the splits have been run.\n",
    "                score.append([accuracy, precision, recall, f1, train_time])\n",
    "\n",
    "            averages = np.array(score).mean(axis=0)\n",
    "            \n",
    "            print(f\"{c} {m} {s}: Averages: {averages}\")\n",
    "            \n",
    "av_df = pd.DataFrame(data=averages, index = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"Train time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6b158-554e-464d-9b28-8d5caff2268a",
   "metadata": {},
   "source": [
    "#### Test the New Model With the Validation Set\n",
    "Using the model trained in the previous cell, perform a predict using the assignment validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f48d5f-2969-4f54-98d5-2dd41a453079",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model( model, val_X, val_y )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
